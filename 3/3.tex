\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{../common}
\usepackage{../pagesetup}
% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}

\lecture{3}{September 11}{Sasha Rush}{Christopher Mosch, Lindsey Brown, Ryan Lapcevic}{Multivariate Normal Distributions}

\subsection{Examples}
Multivariate gaussians are used for modeling in various applications, where knowing mean and variance is useful:
\begin{itemize}
\item{radar: mean and variance of approaching objects (like invading aliens)}
\item{weather forecasting: predicting the position of a hurricane, where the uncertainty in the storm's position increases for timepoints farther away}
\item{tracking the likely outcome of a sports game: last year's superbowl is an example of a failure of modeling with multivariate gaussians as the Patriots still won after a large Falcons' lead}
\end{itemize}

\subsection{Review: Eigendecomposition} 

Let $\v \Sigma$ be a square, symmetric matrix. Then its eigendecomposition is given by $\v\Sigma = \v U^T \v\Lambda \v U$, where $\v U$ is an orthogonal matrix and $\v\Lambda$ is a diagonal matrix. In the special case that $\v\Sigma$ is positive semidefinite (as is the case for covariance matrices), denoted $\v\Sigma \succeq 0$, all its eigenvalues are nonnegative, $\Lambda_{ii} \geq 0$, and we can decompose its inverse as $\v\Sigma^{-1} = \v U^T \v\Lambda^{-1}\v U$, where $\Lambda_{ii}^{-1} = 1/\Lambda_{ii}$.

\subsection{Multivariate Normal Distributions (MVNs)}
Let $X$ be a D-dimensional MVN random vector with mean $\v\mu$ and covariance matrix $\v\Sigma$, denoted $X \sim \mathcal{N}\left(\v\mu,\v\Sigma\right)$. Then the pdf of $X$ is
$$ p(x) = \left(2\pi\right)^{-D/2} |\v\Sigma|^{-1/2} \exp{[-\frac{1}{2}(x-\v\mu)^T\v\Sigma^{-1}(x-\v\mu)]}, $$
where for many problems we focus on the quadratic form $(x-\v\mu)^T\v\Sigma^{-1}(x-\v\mu)$ (which geometrically can be thought of as distance) and ignore the normalization factor $\left(2\pi\right)^{-D/2} |\v\Sigma|^{-1/2}$. Note that MVNs are identifiable using just their first and second moments. %The figure below plots the contours of a bivariate Normal for various $\v\mu$ and $\v\Sigma$ (in the figure, $\rho$ denotes the off-diagonal elements of $\v\Sigma$, given by the covariance of $x_1$ and $x_2$).

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{MVN_contour.png}
\caption{Contours of a bivariate Normal for various $\v\mu$ and $\v\Sigma$ (in the figure, $\rho$ denotes the off-diagonal elements of $\v\Sigma$, given by the covariance of $x_1$ and $x_2$)}\label{fig1}
\end{figure}

Note that we can decompose $\v\Sigma$ as
\begin{align*}
\v\Sigma &= (x-\v\mu)^T\v\Sigma^{-1}(x-\v\mu) \\
&= (x-\v\mu)^T\left(\v U^T \v\Lambda^{-1}\v U\right)(x-\v\mu) \\
&= (x-\v\mu)^T\left(\sum_d{\frac{1}{\lambda_d} U_d U^T}\right)(x-\v\mu) \\
&= \sum_d{\frac{1}{\lambda_d}(x-\v\mu)^T U_d U_d^T (x-\v\mu)},
\end{align*}
where $(x-\v\mu)^T U_d$ can be interpreted as the projection of $(x-\v\mu)$ onto $U_d$ (which can each be thought of as univariate gaussians), the eigenvector corresponding to the eigenvalue $\lambda_d$. Since $\v\Sigma$ is the weighted sum of the dot product of such projections (with weights being given by $1/\lambda_d$, which can be thought of as the scale $1/\sigma^2$), we can describe the MVN as tiling of univariates.

\subsubsection{Manipulating MVNs: Stretches, Rotations, and Shifts}
Let $x\sim\mathcal{N}(0,\v I)$ and $y = \v A x + \v b$. We want to consider two ways of obtaining the complete distribution of $y$.

\begin{itemize}
\item 'Overkill': We can perform a change of variables\footnote{A change of variables can be done in the following way: Let $y=f(x)$ and assume $f$ is invertible so that $x=f^{-1}(y)$. Then $p(y) = p(x)|dx/dy|$.  This is a technique which will be used often in this course.}. Here, we have $x = \v A^{-1}$ and $|dx/dy| = \v |A^{-1}|$, leading to
\begin{align*}
p(y) &= \mathcal{N}\left(\v A^{-1}(y-\v b)|0,\v I\right)|\v A^{-1}| \\
&= \frac{1}{z}\exp{[(\v A^{-1}(y-\v b))^T(\v A^{-1}(y-\v b))]} \\
&= \frac{1}{z}\exp{[(y-\v b)^T(\v A^{-1})^T(\v A^{-1})(y-\v b)]} \\
&= \mathcal{N}(y|\v b,\v A\v A^T),
\end{align*}
where $z$ is the normalizing constant.
\item Using the properties of MVN, we know that $y$ is also MVN, so is completely specified by its mean and covariance matrix which can easily be derived,
$$ \E(y) = \E(\v A x + \v b) = \v A \E(x) + \v b \quad\quad \cov(y) = \v A \v A^T. $$
\end{itemize}

Thus, we can generate MVN from $\mathcal{N}(0,\v I)$ via the transformation $y = \v A x + \v b$, where we set $\v A = \v U \v \Lambda^{1/2}$, leading to $\v\Sigma_Y = \v U^T \v\Lambda \v U$. Then shifts are represented by $\v b$, stretches by $\v \Lambda$, and rotations by $\v U$.

\subsubsection{Detour: MVN in High-Dimensions ($D\gg 0$)}
Let $x$ be a D-dimensional random vector, distributed as $\mathcal{N}(0,I/D)$, where $I$ is the identity. The expected length of $x$ is given by
$$ \E\left(\lVert x \rVert^2\right) = \E\left(\sum_d{x_d^2}\right) = D\sigma_d^2 = 1, $$
which means that $x$ is expected to be on the boundary of a unit sphere centered at the origin. Moreover, the variance of the length is
$$ \text{var}\left({\lVert x \rVert}^2 \right) = D\cdot\left(\E(x^4) - \E\left(x^2\right)^2\right) = D\cdot(3\sigma^4-\sigma^4) = 2D/D^2 = 2/D$$
Thus, it is not only expected that $x$ lies on the boundary but as $D$ increases most of its realizations will in fact fall on the boundary\footnote{It is left as an exercise to show that this formula holds.  Hint: Use the fact that we assumed no covariance.}. A good illustration can be found at \url{http://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/}.

\subsubsection{Key Formulas for MVN: Marginalization and Conditioning}
Let $X \sim \mathcal{N}\left(\v\mu,\v\Sigma\right)$ with
$$ X = \begin{pmatrix}X_1\\X_2\end{pmatrix}, \quad 
\v{\mu} = \begin{pmatrix}\v{\mu}_1\\ \v{\mu}_2\end{pmatrix}, \quad
\v\Sigma = \begin{pmatrix}
\v\Sigma_{11} & \v\Sigma_{12} \\
\v\Sigma_{21} & \v\Sigma_{22}
\end{pmatrix}.
$$
Note that $\Sigma$ is written in block matrix form, rather than scalar entries.  It turns out that the marginals, $X_1$ and $X_2$, are also MVN, and their mean and covariance matrice are given by $\v\mu_1$ and $\v\Sigma_{11}$ and $\v\mu_2$ and $\v\Sigma_{22}$ respectively. A sketch of the proof is provided below.
$$ p(x_1) = \int_{x_2}{N(x|\v\mu,\v\Sigma)dx_2}, $$
which can be written as
$$ 0.5\int_{x_2}{\exp{\left[(x_1-\v\mu_1)^T\v\Sigma_{11}^{-1}(x_1-\mu_1) 
+ 2(x_1-\v\mu_1)^T\v\Sigma_{12}^{-1}(x_2-\mu_2) + 
(x_2-\v\mu_2)^T\v\Sigma_{22}^{-1}(x_2-\mu_2)\right]}dx_2}. $$
Note that this equals
$$ p(x_1)\int_{x_2}{p(x_2|x_1)dx_2}, $$
implying that $X_1\sim\mathcal{N}(\v\mu_1,\v\Sigma_{11})$. 

While the marginals have a simple form, the conditionals are more complicated.  (For a complete derivation, which requires matrix inversion lemmas, refer to Murphy 4.3.1 and 4.3.4.3.) It can be shown that $ X_1|X_2 \sim \mathcal{N}\left(\v\mu_{1|2},\v\Sigma_{1|2}\right)$ with
$$ \v\mu_{1|2} = \v\mu_1+\v\Sigma_{12}\v\Sigma_{22}^{-1}(x_2-\v\mu_2), \quad
\v\Sigma_{1|2} = \v\Sigma_{11}-\v\Sigma_{12}\v\Sigma_{22}^{-1}\v\Sigma_{21}.$$
\subsubsection{Information Form}
An alternative parametrization, called information form, uses the precision matrix (inverse variance) $\v\Lambda = \v\Sigma^{-1}$. 
Suppose $\v x\sim\mathcal{N}\left(\v\mu,\v\Sigma\right)$, then the mean vector $\v\mu$ and the covariance matrix $\v\Sigma$ are called the \textbf{moment parameters} of the distribution. Sometimes it is useful to use the \textbf{canonical parameters} or \textbf{natural parameters}, defined as
$$ \v\Lambda= \v\Sigma^{-1},\;\v\xi=\v\Sigma^{-1}\v\mu $$
We can convert back to the moment parameters using
$$ \v\mu=\v\Lambda^{-1}\v\xi,\;\v\Sigma = \v\Lambda^{-1} $$
Using the canonical parameters, we can write the MVN in \textbf{information form} (i.e., in exponential family form, see Murphy Section 9.2):
$$ \mathcal{N}_c\left(\v x|\v\xi,\v\Lambda\right)=(2\pi)^{-D/2}|\v\Lambda|^{\frac{1}{2}}\exp\left[-\frac{1}{2}\left(\v x^\intercal \v\Lambda\v x+\v\xi^\intercal\v\Lambda^{-1}\v\xi-2\v x^\intercal\v\xi\right)\right] $$
where we use the notation $\mathcal{N}_c()$ to distinguish from the moment parameterization $\mathcal{N}()$. Partitioning $\Lambda$ as
$$ \v\Lambda = \begin{pmatrix}
\v\Lambda_{11} & \v\Lambda_{12} \\
\v\Lambda_{21} & \v\Lambda_{22}
\end{pmatrix}, $$
we can derive the marginalization and conditioning formulas in information form:
\begin{align*}
p(\v x_2) =&\; \mathcal{N}_c\biggl(\v x_2|\v\xi_2-\v\Lambda_{21}\v\Lambda_{11}^{-1}\v\xi_1,\v\Lambda_{22}-\v\Lambda_{21}\v\Lambda_{11}^{-1}\v\Lambda_{12}\biggr)\\
p(\v x_1|\v x_2)=&\; \mathcal{N}_c\biggl(\v x_1|\v \xi_1 - \v\Lambda_{12}\v x_2,\v\Lambda_{11}\biggr).
\end{align*}
The covariance matrices of the conditional distributions have a simple form. For example, the covariance matrix of $\v x_1$ given $\v x_2$ is $\v\Lambda_{1|2}=\v\Lambda_{11}$. However, the simplicity of the conditional precision comes at the cost of marginalization (which was simple when using $\v\Sigma$) becoming a more complicated expression (see Murphy subsection 4.3 for more details).

%\newpage
\vspace{0.4in}

\begin{exercise}
See hint 2.
\end{exercise}

\begin{proof}
$x$ is a D-dimensional random vector, distributed as $\mathcal{N}(0,I/D)$, where $I$ is the identity. So for each dimension $d\in \{1,2,\ldots,D\}$, we have $x_d\sim\mathcal{N}(0,1/D)$ independently. Thus the expected length of $x$ is
\begin{align*}
\E\left(\lVert x \rVert^2\right) = \E\left(\sum_d{x_d^2}\right) = \sum_{d} \E\left(x_d^2\right) = \sum_{d} 1/D = D\cdot 1/D = 1
\end{align*}

which means that $x$ is expected to be on the boundary of a unit sphere centered at the origin.

Moreover, we have
\begin{align*}
\E(\|x\|^4) =& \E\left(\left(\sum_{d}x_d^2\right)^2\right)=\E\left(\sum_d x_d^4 + \sum_{d}\sum_{d':d'\neq d}x_d^2 x_{d'}^2\right)\\
=& \sum_{d}\E\left(x_d^4\right) + \sum_{d}\sum_{d':d'\neq d}\E\left(x_d^2 x_{d'}^2\right)\\
=& \sum_{d} 3/D^2 + \sum_{d}\sum_{d':d'\neq d} \E\left(x_d^2\right)\E\left(x_{d'}^2\right)\qquad\quad\text{since }x_d{\perp\!\!\!\perp} x_{d'}\\
=& 3/D + \sum_{d}\sum_{d':d'\neq d} 1/D^2\\
=& 3/D + D(D-1)\cdot 1/D^2\\
=& 3/D + (D-1)/D\\
=& 2/D + 1
\end{align*}

Therefore, the variance of the length is
$$ \text{var}\left({\lVert x \rVert}^2 \right) = \E\left(\|x\|^4\right)-\left(\E(\|x\|^2)\right)^2=
2/D + 1 - 1 = 2/D \rightarrow 0 \quad\text{as }\;D\rightarrow\infty$$
Thus, it is not only expected that $x$ lies on the boundary but as $D$ increases most of its realizations will in fact fall on the boundary.
\end{proof}


\end{document}



